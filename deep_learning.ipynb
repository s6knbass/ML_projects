{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c7a6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DL(object):\n",
    "    \"\"\"\n",
    "    A deep learning class used for L-layer neural networks\n",
    "    ***WIP***\n",
    "    \"\"\"  \n",
    "    \n",
    "    ###constructor\n",
    "    def __init__(self, size, input_layer, true_values, testing_input, testing_true, layers_dims, learning_rate = 0.0075, hidden_function = 'relu', last_function = 'sigmoid', num_iterations = 3000, output_form = 'binary', print_cost = False, plot_cost = True):\n",
    "        self.size = size\n",
    "        self.X = input_layer\n",
    "        self.Y = true_values\n",
    "        self.test_X = testing_input\n",
    "        self.test_Y = testing_true\n",
    "        self.learning_rate = learning_rate\n",
    "        self.last_function = last_function\n",
    "        self.hidden_function = hidden_function\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_cost = print_cost\n",
    "        self.dimensions = np.array(layers_dims)\n",
    "        self.final_paramters = {}\n",
    "        self.output_form = output_form\n",
    "        self.plot_cost_bool = plot_cost\n",
    "        self.optimal_dimensions = np.zeros_like(layers_dims)\n",
    "        self.index_bool = False\n",
    "        self.L2 = False\n",
    "        self.lambd = 0.1\n",
    "        self.dropout = False\n",
    "        self.keep_prob = 0.5\n",
    "\n",
    "    ##helper function\n",
    "    def help_me(self):\n",
    "        print(\" mandatory initializer variables: \", \n",
    "        \"    \\033[1m size (int):\\033[0m the number of layers including input and output layers\",\n",
    "        \"    \\033[1m input_layer (array):\\033[0m the array of input variables for every instance\",\n",
    "        \"        every column is seen as one data group where the number of rows represent the\",\n",
    "        \"        number of nodes present in the input layer\"\n",
    "        \"    \\033[1m true_values (array):\\033[0m the array of true values to train with\",\n",
    "        \"    \\033[1m testing_input (array):\\033[0m in the same form as input-layer, used to test the final solution\",\n",
    "        \"    \\033[1m testing_true (array):\\033[0m in the same form as output-layer, used to test the final solution\",\n",
    "        \"    \\033[1m dimensions (array):\\033[0m array of dimensions of each layer\",\n",
    "        \"   optional variables for initializer:\\033[0m\", \n",
    "        \"    \\033[1m learning_rate (float):\\033[0m defualt set to 0.0075 determines step size of gradient descent\",\n",
    "        \"    \\033[1m hidden_function (string):\\033[0m default set to 'relu', determines activation function of hidden layer\",\n",
    "        \"    \\033[1m last_function (string):\\033[0m default set to 'sigmoid', determines activation function of output layer\",\n",
    "        \"    \\033[1m num_iteration (int):\\033[0m default set to 3000, determines number of iterations in gradient descent phase\",\n",
    "        \"    \\033[1m print_cost (boolean):\\033[0m default set to False, when true prints the cost at every 100th iteration\",\n",
    "        \"    \\033[1m plot_cost (boolean):\\033[0m default set to True, when true makes plot of output of cost function\",\n",
    "        \"\\033[1m set_dimensions(array layers_dims):\\033[0m sets the dimensions (number of nodes) of each layer\",\n",
    "        \"\\033[1m set_learning_rate(int rate):\\033[0m set the learning rate for gradient descent\",\n",
    "        \"\\033[1m set_hidden_function(string function):\\033[0m set the function for the hidden layer. options:\",\n",
    "        \"    \\033[1m 'relu':\\033[0m use the Rectified Linear Unit\",\n",
    "        \"    \\033[1m 'sigmoid':\\033[0m use the sigmpoid function\",\n",
    "        \"    \\033[1m 'softmax':\\033[0m use the Softmax function ***WIP***\",\n",
    "        \"\\033[1m set_last_function(string function):\\033[0m set the function for the hidden layer. options same as for set_hidden_function\",\n",
    "        \"\\033[1m set_num_iterations(int num):\\033[0m set the number of iterations for the gradient descent\",\n",
    "        \"\\033[1m set_print_cost(bool value):\\033[0m turn on/off printing every 100th iteration of gradient descent\",\n",
    "        \"\\033[1m set_print_cost(bool value):\\033[0m turn on/off plotting the cost function output\",\n",
    "        \"\\033[1m set_final_params(array parameters):\\033[0m manually set final parameters\", \n",
    "        \"\\033[1m run_machine():\\033[0m runs the Machine learning algorithm\",\n",
    "        \"\\033[1m machine_accuracy():\\033[0m compares the accuracy of the machine solution to weights and biases using the training data and the testing data\",\n",
    "        \"\\033[1m set_L2(bool value, float num):\\033[0m turn on/off using L2 regularization default: False and set lambda value (default 0.1)\",\n",
    "        \"\\033[1m set_dropout(bool value, float num):\\033[0m turn on/off jusing Dropout method for regularization default: False and set the dropout probability (default 0.5)\",\n",
    "        sep = \"\\n\")\n",
    "\n",
    "    ##methods for updating varius variables which can also be initialised in the constructor\n",
    "    def set_dimensions(self, layers_dims):\n",
    "        if len(layers_dims) != self.size:\n",
    "            print(\"you must enter dimensions for \"+str(size)+\" layers or reset the size\")\n",
    "        else:\n",
    "            self.dimensions = layers_dims\n",
    "\n",
    "    def set_learning_rate(self, rate):\n",
    "        if isinstance(rate, float):\n",
    "            self.learning_rate = rate\n",
    "        else:\n",
    "            print(\"argument must be a float\")\n",
    "        \n",
    "    def set_num_iterations(self, num):\n",
    "        if isinstance(num, int):\n",
    "            self.num_iterations = num\n",
    "        else:\n",
    "            print(\"argument must be an integer\")\n",
    "        \n",
    "    def set_print_cost(self, boo):\n",
    "        if isinstance(boo, bool):\n",
    "            self.print_cost = boo\n",
    "        else:\n",
    "            print(\"argument must be a boolean value\")\n",
    "        \n",
    "    def set_plot_cost(self, boo):\n",
    "        if isinstance(boo, bool):\n",
    "            self.plot_cost = boo\n",
    "        else:\n",
    "            print(\"argument must be a boolean value\")\n",
    "        \n",
    "    def set_final_params(self, params):\n",
    "        if isinstance(params, list):\n",
    "            self.final_parameters = params\n",
    "        else:\n",
    "            print(\"argument should be an array of values\")\n",
    "            \n",
    "    def set_output_form(self, form):\n",
    "        if ((form != 'binary') and (form != 'multiclass')):\n",
    "            print(\"argument must be either 'binary' or 'multiclass'\")\n",
    "        else:\n",
    "            self.output_form = form\n",
    "    \n",
    "    def set_hidden_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.hidden_function = function\n",
    "\n",
    "    def set_last_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.last_function = function\n",
    "        \n",
    "    def set_L2(self, boo, lambd = 0.1):\n",
    "        if isinstance(lambd, float):\n",
    "            self.L2 = True\n",
    "            self.lambd = lambd\n",
    "        else:\n",
    "            print(\"argument must be in the form (bool, float) value\")\n",
    "            \n",
    "    def set_dropout(self, boo, prob = 0.5):\n",
    "        if isinstance(boo, bool):\n",
    "            self.dropout = boo\n",
    "            self.keep_prob = prob\n",
    "        else:\n",
    "            print(\"argument must be a boolean value\")\n",
    "            \n",
    "    ## activation functions used for the activation step of each layer\n",
    "    ## currently relu and sigmoid are functional\n",
    "    ## TODO soft_max\n",
    "    def __sigmoid(self, Z):\n",
    "        #print(\"started sigmoid\")\n",
    "        Z = Z.astype(float)\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        activation_cache = Z\n",
    "        #print(\"finished sigmoid\")\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __relu(self, Z):\n",
    "        #print(\"started relu\")\n",
    "        A = np.maximum(0,Z)\n",
    "        activation_cache = Z\n",
    "        #print(\"finished relu\")\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __soft_max(self, Z):\n",
    "        num_rows, num_columns = Z.shape\n",
    "        activation_cache = Z\n",
    "        A = Z*0\n",
    "        exps = np.exp(Z)\n",
    "        exps_sums = np.zeros((1,Z.shape[1]))\n",
    "        exps_sums = np.sum(exps, axis = 0)\n",
    "        for i in range(num_columns):\n",
    "            for j in range(num_rows):\n",
    "                A[j][i] = exps[j][i]/exps_sums[i]\n",
    "        return A, activation_cache\n",
    "\n",
    "    ## backwards propagation step for the activation of each layer\n",
    "    ## currently the sigmoid and relu functions are functional\n",
    "    ## TODO soft_max\n",
    "    def __sigmoid_backward(self, dA, activation_cache):\n",
    "        #print(\"started sigmoid back\")\n",
    "        S, Z = self.__sigmoid(activation_cache)\n",
    "        dZ = dA * (S*(1.0-S))\n",
    "        #print(\"finished sigmoid back\")\n",
    "        return dZ\n",
    "    \n",
    "    def __relu_backward(self, dA, activation_cache):\n",
    "        #print(\"started relu back\")\n",
    "        Z = activation_cache\n",
    "        num_rows, num_columns = Z.shape\n",
    "        dZ = Z * 0\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_columns):\n",
    "                if Z[i][j] <= 0:\n",
    "                    dZ[i][j] = 0\n",
    "                elif Z[i][j] > 0:\n",
    "                    dZ[i][j] = dA[i][j]\n",
    "        #print(\"finished relu back\")\n",
    "        return dZ\n",
    "    \n",
    "    def __soft_max_backward(self, dA, activation_cache):\n",
    "        S, Z = self.__soft_max(activation_cache)\n",
    "        m, n = Z.shape\n",
    "        p = soft_max(Z)\n",
    "        # outer products\n",
    "        # (p1^2  p1*p2 p1*p3 ...)\n",
    "        # (p2*p1 p2^2  p2*p3 ...)\n",
    "        # (...                  )\n",
    "        tensor1 = np.einsum('ij,ik->ijk',p,p)\n",
    "        # (n,n) identitity of feature vector\n",
    "        # (p1 0  0 ...)\n",
    "        # (0  p2 0 ...)\n",
    "        # (...        )\n",
    "        tensor2 = np.einsum('ij,jk->ijk',p,np.eye(n,n))\n",
    "    \n",
    "        dSoftmax = tensor2 - tensor1\n",
    "        dZ = np.einsum('ijk,ik->ij', dSoftmax, dA)\n",
    "        return dZ\n",
    "\n",
    "    ##initialize the weights and biases for each layer.\n",
    "    ## weights are initialized to random floats using the He initialization\n",
    "    ## biases are initialized to zero\n",
    "    def __initialize_parameters(self, layers_dims):\n",
    "        #print(\"started initialize params\")\n",
    "        np.random.seed(1)\n",
    "        parameters = {}\n",
    "        L = len(layers_dims)\n",
    "        \n",
    "        for l in range(1,L):\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * math.sqrt(2/(layers_dims[l-1]))\n",
    "            parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layers_dims[l], 1))\n",
    "        #print(\"finished initialize params\")\n",
    "        return parameters\n",
    "    \n",
    "    ## linear forward propogation step, taken at each layer\n",
    "    def __linear_forward(self, A, W, b):\n",
    "        #print(\"started linear forward\")\n",
    "        Z = np.dot(W,A) + b\n",
    "        cache = (A,W,b)\n",
    "        #print(\"finished linear forward\")\n",
    "        return Z, cache\n",
    "    \n",
    "    ## forward propagation for activation step taken at each layer\n",
    "    def __linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        #print(\"started linear activation forward\")\n",
    "        if activation == \"sigmoid\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__sigmoid(Z)\n",
    "        \n",
    "        elif activation == \"relu\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__relu(Z)\n",
    "            if self.dropout:\n",
    "                D = np.random.rand(A.shape[0], A.shape[1])\n",
    "                D = (D < self.keep_prob).astype(int)\n",
    "                print(A.shape)\n",
    "                print(D.shape)\n",
    "                A = np.multiply(A,D)\n",
    "                A = A/self.keep_prob\n",
    "                cache = (linear_cache, activation_cache, D)\n",
    "                return A, cache\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__soft_max(Z)\n",
    "        #print(A.shape)\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        #print(\"finished linear activation forward\")\n",
    "    \n",
    "        return A, cache\n",
    "    \n",
    "    ## forward propagation for the model (includes the linear and activation step for every layer)\n",
    "    def __full_forward(self, X, parameters):\n",
    "        #print(\"started full forward\")\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = self.size -1\n",
    "    \n",
    "        if self.dropout:\n",
    "            for l in range(1, L):\n",
    "                print(l)\n",
    "                A_prev = A\n",
    "                A, cache = self.__linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = self.hidden_function)\n",
    "                caches.append(cache)\n",
    "            AL, cache = self.__linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = self.last_function)\n",
    "            caches.append(cache)\n",
    "        else:\n",
    "            for l in range(1, L):\n",
    "                A_prev = A\n",
    "                A, cache = self.__linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = self.hidden_function)\n",
    "                caches.append(cache)\n",
    "            AL, cache = self.__linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = self.last_function)\n",
    "            caches.append(cache)\n",
    "        #print(\"finished full forward\")\n",
    "        return AL, caches\n",
    "    \n",
    "        ## cross-entropy cost function\n",
    "        ## L2 regularization option\n",
    "    def __compute_cost(self, AL, Y, parameters):\n",
    "        #print(\"started compute cost\")\n",
    "        L = self.size -1\n",
    "        L2_cost = 0\n",
    "        m = Y.shape[1]*Y.shape[0]\n",
    "        cross_entropy_cost = -(np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)))/m\n",
    "        if self.L2:\n",
    "            for l in range(1,L+1):\n",
    "                L2_cost += np.sum(np.square(parameters[\"W\"+str(l)])) * (self.lambd/(2*m))\n",
    "        cost = cross_entropy_cost + L2_cost\n",
    "        cost = np.squeeze(cost)\n",
    "        #print(\"finished compute cost\")\n",
    "        return cost\n",
    "    \n",
    "    ## linear backward propogation step, taken at each layer\n",
    "    ## this includes the dlambd term from L2 regularization in cost function\n",
    "    def __linear_backward(self, dZ, cache):\n",
    "        #print(\"started linear backward\")\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "        dlambd = 0.\n",
    "        if self.L2:\n",
    "            dlambd = (self.lambd/m) * W\n",
    "        \n",
    "        dW = (np.dot(dZ,A_prev.T))/m + dlambd\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        #print(\"finished linear backward\")\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## backward propagation for activation step taken at each layer\n",
    "    def __linear_activation_backward(self, dA, cache, activation):\n",
    "        #print(\"started linear activation backward\")\n",
    "    \n",
    "        if activation == \"relu\":\n",
    "            if self.dropout:\n",
    "                linear_cache, activation_cache, D = cache\n",
    "                dZ = self.__relu_backward(dA, activation_cache)\n",
    "                dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "                dA_prev = (dA_prev*D)/self.keep_prob\n",
    "            else:\n",
    "                linear_cache, activation_cache = cache\n",
    "                dZ = self.__relu_backward(dA, activation_cache)\n",
    "                dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            linear_cache, activation_cache = cache\n",
    "            dZ = self.__sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            linear_cache, activation_cache = cache\n",
    "            dZ = self.__soft_max_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "        #print(\"finished linear activation backward\")\n",
    "        #print(dA_prev.shape)\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## backward propagation for the model (includes the linear and activation step for every layer)\n",
    "    def __full_backward(self, AL, Y, caches):\n",
    "        #print(\"started full backward\")\n",
    "        grads = {}\n",
    "        L = self.size - 1 #num layers\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "    \n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = self.__linear_activation_backward(dAL, current_cache, activation = self.last_function)\n",
    "        grads[\"dA\"+str(L-1)] = dA_prev_temp\n",
    "        grads[\"dW\"+str(L)] = dW_temp\n",
    "        grads[\"db\"+str(L)] = db_temp\n",
    "    \n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.__linear_activation_backward(dA_prev_temp, current_cache, activation= self.hidden_function)\n",
    "            grads[\"dA\"+str(l)] = dA_prev_temp\n",
    "            grads[\"dW\"+str(l+1)] = dW_temp\n",
    "            grads[\"db\"+str(l+1)] = db_temp\n",
    "        #print(\"finished full backward\")\n",
    "        return grads\n",
    "\n",
    "    ## update parameters using gradient descent\n",
    "    def __update_parameters(self, params, grads, learning_rate):\n",
    "        #print(\"started update parameters\")\n",
    "        parameters = params.copy()\n",
    "        L = len(parameters) // 2 #num layers\n",
    "    \n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        #print(\"finished update parameters\")\n",
    "        return parameters\n",
    "    \n",
    "    ## plots the value of the cost function for every 100th iteration of the gradient descent\n",
    "    def __plot_costs(self, costs):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "    ## main function to implement the gradient descent learning algorithm\n",
    "    def __deep_model(self, X, Y):\n",
    "        #print(\"started deep model\")\n",
    "        np.random.seed(1)\n",
    "        costs = []\n",
    "    \n",
    "        parameters = self.__initialize_parameters(self.dimensions)\n",
    "        for i in range(0, self.num_iterations):\n",
    "            AL, caches = self.__full_forward(X, parameters)\n",
    "            cost = self.__compute_cost(AL, Y, parameters)\n",
    "            grads = self.__full_backward(AL, Y, caches)\n",
    "            parameters = self.__update_parameters(parameters, grads, self.learning_rate)\n",
    "            if self.print_cost and (i%100 == 0 or 1 == self.num_iterations - 1):\n",
    "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            if i % 100 == 0 or i == self.num_iterations:\n",
    "                costs.append(cost)\n",
    "                #print(i)\n",
    "        #print(\"finished deep model\")\n",
    "        return parameters, costs\n",
    "\n",
    "    ## can show the accuracy with which the final set-up can predict the outcomes based on the inputs\n",
    "    ## option for 'binary' true values of 0,1 and 'multiclass' true values of arrays i,e [0, 1, 0, 0]\n",
    "    def __predict(self, X, Y, parameters, option = \"binary\", print_accuracy = True):\n",
    "        print(\"started predict\")\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = self.__linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = self.hidden_function)\n",
    "        AL, cache = self.__linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = self.last_function)\n",
    "    \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if option == \"binary\":\n",
    "            AL = np.round(AL)\n",
    "            num_rows, num_columns = AL.shape\n",
    "            for i in range(num_rows):\n",
    "                for j in range(num_columns):\n",
    "                    if AL[i][j] == Y[i][j]:\n",
    "                        correct += 1\n",
    "                    total +=1\n",
    "                \n",
    "        if option == \"multiclass\":\n",
    "            Y_arg = np.argmax(Y, axis = 0)\n",
    "            AL_arg = np.argmax(AL, axis = 0)\n",
    "            for i in range(len(Y_arg)):\n",
    "                if Y_arg[i] == AL_arg[i]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        if print_accuracy:\n",
    "            print(\"total = \"+str(total)+\" correct = \"+str(correct))\n",
    "            print(\"Accuracy = \"+str(correct/total))\n",
    "        print(\"finished predict\")\n",
    "        return correct/total\n",
    "    \n",
    "    ## this will run the machine, deterining the parameters which minimize the cost\n",
    "    def run_machine(self):\n",
    "        #print(\"started run machine\")\n",
    "        self.final_parameters, costs = self.__deep_model(self.X, self.Y)\n",
    "        if self.plot_cost_bool:\n",
    "            self.__plot_costs(costs)\n",
    "        #print(\"ended run machine\")\n",
    "        \n",
    "    ## with this function, the accuracy of the machine using training and test data are calculate\n",
    "    def machine_accuracy(self, test_x = None, test_y = None, option = None, print_accuracy = True):\n",
    "        print(\"started machine_accuracy\")\n",
    "        if test_x == None:\n",
    "            test_x = self.test_X\n",
    "        if test_y == None:\n",
    "            test_y = self.test_Y\n",
    "        if option == None:\n",
    "            option = self.output_form\n",
    "        self.__predict(self.X, self.Y, self.final_parameters, option, print_accuracy)\n",
    "        self.__predict(test_x, test_y, self.final_parameters, option, print_accuracy)\n",
    "        print(\"finsihed machine accuracy\")\n",
    "    \n",
    "    def __optimization_loop(self, index, max_size, num_layers, accuracy_high):\n",
    "        for i in range(1, max_size + 1):\n",
    "            #print(\"current index value is: \"+str(index))\n",
    "            if index == 0:\n",
    "                return\n",
    "            else:\n",
    "                self.dimensions[index] = i\n",
    "                print(self.dimensions)\n",
    "                self.run_machine()\n",
    "                accuracy = self.__predict(self.test_X, self.test_Y, self.final_parameters, option=\"multiclass\")\n",
    "                if round(accuracy_high, 5) < round(accuracy, 5):\n",
    "                    print(\"****new highest accuracy****\")\n",
    "                    accuracy_high = accuracy\n",
    "                    optimal_dimensions = np.array(self.dimensions)\n",
    "                    self.optimal_dimensions = optimal_dimensions\n",
    "                    print(\"new optimal dimensions = \"+str(self.optimal_dimensions))\n",
    "                #print(\"current optimal dimensions: \"+str(self.optimal_dimensions))\n",
    "                if i == max_size:\n",
    "                    print(self.dimensions)\n",
    "                    self.__check_index(index, max_size)\n",
    "                    if (self.index_bool == True):\n",
    "                        self.index_bool = False\n",
    "                        self.__optimization_loop(index, max_size, num_layers, accuracy_high)\n",
    "                    else:\n",
    "                        return\n",
    "                    \n",
    "    def __check_index(self, number, max_size):\n",
    "        if number == 1:\n",
    "            self.index_bool = False\n",
    "            return\n",
    "        if self.dimensions[number - 1] < max_size:\n",
    "            self.dimensions[number - 1] += 1\n",
    "            self.index_bool = True\n",
    "            return\n",
    "        elif self.dimensions[number - 1] == max_size:\n",
    "            number -=1\n",
    "            if number == 0:\n",
    "                return\n",
    "            self.dimensions[number] = 1\n",
    "            self.__check_index(number, max_size)\n",
    "    \n",
    "    ## this function will iterate through possible dimensions of hidden layers to guess at the optimal\n",
    "    ## dimension length\n",
    "    ## TODO: make optimizations for each of the other variables\n",
    "    def optimize_dimensions(self, max_size):\n",
    "        n_x = self.dimensions[0]\n",
    "        n_y = self.dimensions[-1]\n",
    "        num_layers = len(self.dimensions) - 2\n",
    "        index = num_layers\n",
    "        accuracy_high = 0\n",
    "        for i in range(1,num_layers + 1):\n",
    "            self.dimensions[i] = 1\n",
    "        self.__optimization_loop(index, max_size, num_layers, accuracy_high)\n",
    "        #print(\"The optimal dimensions are: \"+str(self.optimal_dimensions)+\"\\n\\n\")\n",
    "        return self.optimal_dimensions\n",
    "    \n",
    "    \n",
    "    def run_optimal(self, max_size, plot_costs = True):\n",
    "        self.plot_cost_bool = plot_costs\n",
    "        self.dimensions = self.optimize_dimensions(max_size)\n",
    "        #print(\"the optimal dimensions are :\"+str(self.dimensions)+\" for the given max layer dimension: \"+str(max_size))\n",
    "        self.run_machine()\n",
    "        self.machine_accuracy(print_accuracy = True)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fe1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
