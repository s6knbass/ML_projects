{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f19278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39549d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML(object):\n",
    "    \"\"\"\n",
    "    class for implementing the machine learning process\n",
    "    used for the 2 layer case.  A deep learning version is coming soon\n",
    "    \"\"\"\n",
    "    \n",
    "    ## constructor\n",
    "    def __init__(self, size, input_layer, true_values, testing_input, testing_true, layers_dims, learning_rate = 0.0075, hidden_function = 'relu', last_function = 'sigmoid', num_iterations = 3000, print_cost = False, plot_cost = True):\n",
    "        self.size = size\n",
    "        self.X = input_layer\n",
    "        self.Y = true_values\n",
    "        self.test_X = testing_input\n",
    "        self.test_Y = testing_true\n",
    "        self.dimensions = layers_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.last_function = last_function\n",
    "        self.hidden_function = hidden_function\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_cost = False\n",
    "        self.final_paramters = {}\n",
    "        self.plot_cost_bool = plot_cost\n",
    "        \n",
    "    ##function to list all possible commands\n",
    "    def help_me(self):\n",
    "        print(\" mandatory initializer variables: \", \n",
    "        \"    \\033[1m size (int):\\033[0m the number of layers including input and output layers\",\n",
    "        \"    \\033[1m input_layer (array):\\033[0m the array of input variables for every instance\", \n",
    "        \"    \\033[1m true_values (array):\\033[0m the array of true values to train with\",\n",
    "        \"    \\033[1m testing_input (array):\\033[0m in the same form as input-layer, used to test the final solution\",\n",
    "        \"    \\033[1m testing_true (array):\\033[0m in the same form as output-layer, used to test the final solution\",\n",
    "        \"   optional variables for initializer:\\033[0m\", \n",
    "        \"    \\033[1m layers_dims (array):\\033[0m array of dimensions of each layer\",\n",
    "        \"    \\033[1m learning_rate (float):\\033[0m defualt set to 0.0075 determines step size of gradient descent\",\n",
    "        \"    \\033[1m hidden_function (string):\\033[0m default set to 'relu', determines activation function of hidden layer\",\n",
    "        \"    \\033[1m last_function (string):\\033[0m default set to 'sigmoid', determines activation function of output layer\",\n",
    "        \"    \\033[1m num_iteration (int):\\033[0m default set to 3000, determines number of iterations in gradient descent phase\",\n",
    "        \"    \\033[1m print_cost (boolean):\\033[0m default set to False, when true prints the cost at every 100th iteration\",\n",
    "        \"    \\033[1m plot_cost (boolean):\\033[0m default set to True, when true makes plot of output of cost function\",\n",
    "        \"\\033[1m set_layers_dims(array layers_dims):\\033[0m sets the dimensions (number of nodes) of each layer\",\n",
    "        \"\\033[1m set_learning_rate(int rate):\\033[0m set the learning rate for gradient descent\",\n",
    "        \"\\033[1m set_hidden_function(string function):\\033[0m set the function for the hidden layer. options:\",\n",
    "        \"    \\033[1m 'relu':\\033[0m use the Rectified Linear Unit\",\n",
    "        \"    \\033[1m 'sigmoid':\\033[0m use the sigmpoid function\",\n",
    "        \"    \\033[1m 'softmax':\\033[0m use the Softmax function ***WIP***\",\n",
    "        \"\\033[1m set_last_function(string function):\\033[0m set the function for the hidden layer. options same as for set_hidden_function\",\n",
    "        \"\\033[1m set_num_iterations(int num):\\033[0m set the number of iterations for the gradient descent\",\n",
    "        \"\\033[1m set_print_cost(bool value):\\033[0m turn on/off printing every 100th iteration of gradient descent\",\n",
    "        \"\\033[1m set_print_cost(bool value):\\033[0m turn on/off plotting the cost function output\",\n",
    "        \"\\033[1m set_final_params(array parameters):\\033[0m manually set final parameters\", \n",
    "        \"\\033[1m run_machine():\\033[0m runs the Machine learning algorithm\",\n",
    "        \"\\033[1m machine_accuracy():\\033[0m compares the accuracy of the machine solution to weights and biases using the training data and the testing data\",\n",
    "        sep = \"\\n\")\n",
    "        \n",
    "        \n",
    "    ## method for recieving number of dimensions for each layer\n",
    "    ## TODO add a posibility to iterate and find ideal dimensions\n",
    "    def set_layers_dims(self, layers_dims):\n",
    "        if len(layers_dims) != self.size:\n",
    "            print(\"you must enter dimensions for \"+str(size)+\" layers or reset the size\")\n",
    "        else:\n",
    "            self.dimensions = layers_dims\n",
    "            \n",
    "    ##methods to update the various varaibles\n",
    "    def set_learning_rate(self, rate):\n",
    "        self.learning_rate = rate\n",
    "        \n",
    "    def set_num_iterations(self, num):\n",
    "        self.num_iterations = num\n",
    "        \n",
    "    def set_print_cost(self, boo):\n",
    "        self.print_cost = boo\n",
    "        \n",
    "    def set_plot_cost(self, boo):\n",
    "        self.plot_cost = boo\n",
    "        \n",
    "    def set_final_params(self, params):\n",
    "        self.final_parameters = params\n",
    "    \n",
    "    ## set hidden layer activation function if different then default\n",
    "    ## options limited to the functions coded inside\n",
    "    def set_hidden_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.hidden_function = function\n",
    "\n",
    "    ## set output layer activation function if different then default\n",
    "    ## options limited to the functions coded inside\n",
    "    def set_last_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.last_function = function\n",
    "    \n",
    "    ## definitions for the activation layers functions\n",
    "    ## also definitions for the backward propogation using derivitives\n",
    "    ## currently includes relu and sigmoid\n",
    "    ## TODO soft_max\n",
    "    def __sigmoid(self, Z):\n",
    "        Z = Z.astype(float)\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        activation_cache = Z\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __relu(self, Z):\n",
    "        A = np.maximum(0,Z)\n",
    "        activation_cache = Z\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __soft_max(self, Z):\n",
    "        num_rows, num_columns = Z.shape\n",
    "        activation_cache = Z\n",
    "        A = Z*0\n",
    "        exps = np.exp(Z)\n",
    "        exps_sums = np.zeros((1,Z.shape[1]))\n",
    "        exps_sums = np.sum(exps, axis = 0)\n",
    "        for i in range(num_columns):\n",
    "            for j in range(num_rows):\n",
    "                A[j][i] = exps[j][i]/exps_sums[i]\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __sigmoid_backward(self, dA, activation_cache):\n",
    "        S, Z = self.__sigmoid(activation_cache)\n",
    "        dZ = dA * (S*(1.0-S))\n",
    "        return dZ\n",
    "    \n",
    "    def __relu_backward(self, dA, activation_cache):\n",
    "        Z = activation_cache\n",
    "        num_rows, num_columns = Z.shape\n",
    "        dZ = Z * 0\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_columns):\n",
    "                if Z[i][j] <= 0:\n",
    "                    dZ[i][j] = 0\n",
    "                elif Z[i][j] > 0:\n",
    "                    dZ[i][j] = dA[i][j]\n",
    "        return dZ\n",
    "    \n",
    "    def __soft_max_backward(self, dA, activation_cache):\n",
    "        S, Z = self.__soft_max(activation_cache)\n",
    "        m, n = Z.shape\n",
    "        p = soft_max(Z)\n",
    "        # outer products\n",
    "        # (p1^2  p1*p2 p1*p3 ...)\n",
    "        # (p2*p1 p2^2  p2*p3 ...)\n",
    "        # (...                  )\n",
    "        tensor1 = np.einsum('ij,ik->ijk',p,p)\n",
    "        # (n,n) identitity of feature vector\n",
    "        # (p1 0  0 ...)\n",
    "        # (0  p2 0 ...)\n",
    "        # (...        )\n",
    "        tensor2 = np.einsum('ij,jk->ijk',p,np.eye(n,n))\n",
    "    \n",
    "        dSoftmax = tensor2 - tensor1\n",
    "        dZ = np.einsum('ijk,ik->ij', dSoftmax, dA)\n",
    "        return dZ\n",
    "\n",
    "    ## initialises weights and bias variables for a two-layer NN\n",
    "    def __initialize_parameters(self, n_x, n_h, n_y):\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "        b1 = np.zeros((n_h, 1))\n",
    "        W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "        b2 = np.zeros((n_y,1))\n",
    "    \n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "        return parameters\n",
    "    \n",
    "    ## linear forward propogation step, taken at each layer\n",
    "    def __linear_forward(self, A, W, b):\n",
    "        Z = np.dot(W,A) + b\n",
    "        cache = (A,W,b)\n",
    "        return Z, cache\n",
    "    \n",
    "    ## forward propagation for activation step taken at each layer\n",
    "    def __linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__sigmoid(Z)\n",
    "        \n",
    "        elif activation == \"relu\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__relu(Z)\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            Z, linear_cache = self.__linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__soft_max(Z)\n",
    "        \n",
    "        cache = (linear_cache, activation_cache)\n",
    "    \n",
    "        return A, cache\n",
    "    \n",
    "    ## cost function\n",
    "    def __compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]*Y.shape[0]\n",
    "        cost = -(np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)))/m\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    ## linear backward propogation step, taken at each layer\n",
    "    def __linear_backward(self, dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "    \n",
    "        dW = (np.dot(dZ,A_prev.T))/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## backward propagation for activation step taken at each layer\n",
    "    def __linear_activation_backward(self, dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "    \n",
    "        if activation == \"relu\":\n",
    "            dZ = self.__relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = self.__sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            dZ = self.__soft_max_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.__linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## update parameters using gradient descent\n",
    "    def __update_parameters(self, params, grads, learning_rate):\n",
    "        parameters = params.copy()\n",
    "        L = len(parameters) // 2 #num layers\n",
    "    \n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    ## runs the two layer NN \n",
    "    def __two_layer_model(self, X, Y):\n",
    "        np.random.seed(1)\n",
    "        grads = {}\n",
    "        costs = []\n",
    "        m = X.shape[1]\n",
    "        (n_x, n_h, n_y) = self.dimensions\n",
    "        \n",
    "        parameters = self.__initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        for i in range(0, self.num_iterations):\n",
    "            A1, cache1 = self.__linear_activation_forward(X, W1, b1, self.hidden_function)\n",
    "            A2, cache2 = self.__linear_activation_forward(A1, W2, b2, self.last_function)\n",
    "            cost = self.__compute_cost(A2, Y)\n",
    "            dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "            dA1, dW2, db2 = self.__linear_activation_backward(dA2, cache2, self.last_function)\n",
    "            dA0, dW1, db1 = self.__linear_activation_backward(dA1, cache1, self.hidden_function)\n",
    "            grads['dW1'] = dW1\n",
    "            grads['db1'] = db1\n",
    "            grads['dW2'] = dW2\n",
    "            grads['db2'] = db2\n",
    "            \n",
    "            parameters = self.__update_parameters(parameters, grads, self.learning_rate)\n",
    "            \n",
    "            W1 = parameters[\"W1\"]\n",
    "            b1 = parameters[\"b1\"]\n",
    "            W2 = parameters[\"W2\"]\n",
    "            b2 = parameters[\"b2\"]\n",
    "        \n",
    "            if self.print_cost and (i % 100 == 0 or i == self.num_iterations - 1):\n",
    "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            if i % 100 == 0 or i == self.num_iterations:\n",
    "                costs.append(cost)\n",
    "            \n",
    "        return parameters, costs\n",
    "    \n",
    "    def __plot_costs(self, costs):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "    def __predict(self, parameters, X, Y, option=\"binary\", print_accuracy = True):\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        A1, cache1 = self.__linear_activation_forward(X, W1, b1, self.hidden_function)\n",
    "        A2, cache2 = self.__linear_activation_forward(A1, W2, b2, self.last_function)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if option == \"binary\":\n",
    "            A2 = np.round(A2)\n",
    "            num_rows, num_columns = A2.shape\n",
    "            for i in range(num_rows):\n",
    "                for j in range(num_columns):\n",
    "                    if A2[i][j] == Y[i][j]:\n",
    "                        correct += 1\n",
    "                    total +=1\n",
    "                \n",
    "        if option == \"multiclass\":\n",
    "            Y_arg = np.argmax(Y, axis = 0)\n",
    "            A2_arg = np.argmax(A2, axis = 0)\n",
    "            for i in range(len(Y_arg)):\n",
    "                if Y_arg[i] == A2_arg[i]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        if print_accuracy:\n",
    "            print(\"total = \"+str(total)+\" correct = \"+str(correct))\n",
    "            print(\"Accuracy = \"+str(correct/total))\n",
    "        return correct/total\n",
    "    \n",
    "    def run_machine(self):\n",
    "        self.final_parameters, costs = self.__two_layer_model(self.X, self.Y)\n",
    "        if self.plot_cost_bool:\n",
    "            self.__plot_costs(costs)\n",
    "        \n",
    "    def machine_accuracy(self):\n",
    "        self.__predict(self.final_parameters, self.X, self.Y, option=\"multiclass\")\n",
    "        self.__predict(self.final_parameters, self.test_X, self.test_Y, option=\"multiclass\")\n",
    "        \n",
    "    def optimize_dimensions(self, max_size, plot_costs = True):\n",
    "        n_x, n_h, n_y = self.dimensions\n",
    "        accuracy_high = 0\n",
    "        optimal_dimension = 0\n",
    "        for i in range(1, max_size + 1):\n",
    "            self.dimensions = (n_x, i, n_y)\n",
    "            self.run_machine()\n",
    "            accuracy = self.__predict(self.final_parameters, self.test_X, self.test_Y, option=\"multiclass\")\n",
    "            if round(accuracy_high,5) < round(accuracy,5):\n",
    "                accuracy_high = accuracy\n",
    "                optimal_dimension = i\n",
    "        optimal_dims = np.array((n_x, optimal_dimension, n_y))\n",
    "        print(\"optimal dimensions are: \"+str(optimal_dims))\n",
    "        return(optimal_dims)\n",
    "    \n",
    "    def run_optimal(self, max_size, plot_costs = True):\n",
    "        self.plot_cost_bool = plot_costs\n",
    "        self.dimensions = self.optimize_dimensions(max_size)\n",
    "        self.run_machine()\n",
    "        self.machine_accuracy()\n",
    "        \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a0079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
