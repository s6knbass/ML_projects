{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4477bf",
   "metadata": {},
   "source": [
    "This is a project to attempt to use machine learning to solve the integral \n",
    "$\\frac{1}{x^2-x+1}dx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f19278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39549d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML(object):\n",
    "    \"\"\"\n",
    "    class for implementing the machine learning process\n",
    "    This will include the possibility of both a two layer model\n",
    "    as well as an L-layer model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## constructor\n",
    "    def __init__(self, size, input_layer, true_values, testing_input, testing_true, learning_rate = 0.0075, hidden_function = 'relu', last_function = 'sigmoid', num_iterations = 3000, print_cost = False, plot_cost = False):\n",
    "        self.size = size\n",
    "        self.X = input_layer\n",
    "        self.Y = true_values\n",
    "        self.test_X = testing_input\n",
    "        self.test_Y = testing_true\n",
    "        self.learning_rate = learning_rate\n",
    "        self.last_function = last_function\n",
    "        self.hidden_function = hidden_function\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_cost = False\n",
    "        self.dimensions = []\n",
    "        self.final_paramters = {}\n",
    "        self.plot_cost_bool = plot_cost\n",
    "\n",
    "    ## method for recieving number of dimensions for each layer\n",
    "    ## TODO add a posibility to iterate and find ideal dimensions\n",
    "    def set_layers_dims(self, layers_dims):\n",
    "        if len(layers_dims) != self.size:\n",
    "            print(\"you must enter dimensions for \"+str(size)+\" layers or reset the size\")\n",
    "        else:\n",
    "            self.dimensions = layers_dims\n",
    "            \n",
    "    ##methods to update the various varaibles\n",
    "    def set_learning_rate(self, rate):\n",
    "        self.learning_rate = rate\n",
    "        \n",
    "    def set_last_function(self, function):\n",
    "        self.last_function = function\n",
    "        \n",
    "    def set_hidden_function(self, function):\n",
    "        self.hidden_function = function\n",
    "        \n",
    "    def set_num_iterations(self, num):\n",
    "        self.num_iterations = num\n",
    "        \n",
    "    def set_print_cost(self, boo):\n",
    "        self.print_cost = boo\n",
    "        \n",
    "    def set_plot_cost(self, boo):\n",
    "        self.plot_cost = boo\n",
    "        \n",
    "    def set_final_params(self, params):\n",
    "        self.final_parameters = params\n",
    "    \n",
    "    ## set hidden layer activation function if different then default\n",
    "    ## options limited to the functions coded inside\n",
    "    def hidden_layer_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.hidden_function = function\n",
    "\n",
    "    ## set output layer activation function if different then default\n",
    "    ## options limited to the functions coded inside\n",
    "    def last_layer_function(self, function):\n",
    "        if ((function != 'relu') and  (function != 'sigmoid') and  (function != 'softmax')):\n",
    "            print(\"function must be 'relu', 'sigmoid', or 'softmax', new functions may be added in the future\")\n",
    "        else:\n",
    "            self.last_function = function\n",
    "    \n",
    "    ## definitions for the activation layers functions\n",
    "    ## also definitions for the backward propogation using derivitives\n",
    "    ## currently includes relu and sigmoid\n",
    "    ## TODO soft_max\n",
    "    def __sigmoid(self, Z):\n",
    "        Z = Z.astype(float)\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        activation_cache = Z\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __relu(self, Z):\n",
    "        A = np.maximum(0,Z)\n",
    "        activation_cache = Z\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __soft_max(self, Z):\n",
    "        num_rows, num_columns = Z.shape\n",
    "        activation_cache = Z\n",
    "        A = Z*0\n",
    "        exps = np.exp(Z)\n",
    "        exps_sums = np.zeros((1,Z.shape[1]))\n",
    "        exps_sums = np.sum(exps, axis = 0)\n",
    "        for i in range(num_columns):\n",
    "            for j in range(num_rows):\n",
    "                A[j][i] = exps[j][i]/exps_sums[i]\n",
    "        return A, activation_cache\n",
    "    \n",
    "    def __sigmoid_backward(self, dA, activation_cache):\n",
    "        S, Z = self.__sigmoid(activation_cache)\n",
    "        dZ = dA * (S*(1.0-S))\n",
    "        return dZ\n",
    "    \n",
    "    def __relu_backward(self, dA, activation_cache):\n",
    "        Z = activation_cache\n",
    "        num_rows, num_columns = Z.shape\n",
    "        dZ = Z * 0\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_columns):\n",
    "                if Z[i][j] <= 0:\n",
    "                    dZ[i][j] = 0\n",
    "                elif Z[i][j] > 0:\n",
    "                    dZ[i][j] = dA[i][j]\n",
    "        return dZ\n",
    "    \n",
    "    def __soft_max_backward(self, dA, activation_cache):\n",
    "        S, Z = self.__soft_max(activation_cache)\n",
    "        m, n = Z.shape\n",
    "        p = soft_max(Z)\n",
    "        # outer products\n",
    "        # (p1^2  p1*p2 p1*p3 ...)\n",
    "        # (p2*p1 p2^2  p2*p3 ...)\n",
    "        # (...                  )\n",
    "        tensor1 = np.einsum('ij,ik->ijk',p,p)\n",
    "        # (n,n) identitity of feature vector\n",
    "        # (p1 0  0 ...)\n",
    "        # (0  p2 0 ...)\n",
    "        # (...        )\n",
    "        tensor2 = np.einsum('ij,jk->ijk',p,np.eye(n,n))\n",
    "    \n",
    "        dSoftmax = tensor2 - tensor1\n",
    "        dZ = np.einsum('ijk,ik->ij', dSoftmax, dA)\n",
    "        return dZ\n",
    "\n",
    "    ## initialises weights and bias variables for a two-layer NN\n",
    "    def initialize_parameters(self, n_x, n_h, n_y):\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "        b1 = np.zeros((n_h, 1))\n",
    "        W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "        b2 = np.zeros((n_y,1))\n",
    "    \n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "        return parameters\n",
    "    \n",
    "    ## linear forward propogation step, taken at each layer\n",
    "    def linear_forward(self, A, W, b):\n",
    "        Z = np.dot(W,A) + b\n",
    "        cache = (A,W,b)\n",
    "        return Z, cache\n",
    "    \n",
    "    ## forward propagation for activation step taken at each layer\n",
    "    def linear_activation_forward(self, A_prev, W, b, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__sigmoid(Z)\n",
    "        \n",
    "        elif activation == \"relu\":\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__relu(Z)\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = self.__soft_max(Z)\n",
    "        \n",
    "        cache = (linear_cache, activation_cache)\n",
    "    \n",
    "        return A, cache\n",
    "    \n",
    "    ## cost function\n",
    "    def __compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]*Y.shape[0]\n",
    "        cost = -(np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)))/m\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    ## linear backward propogation step, taken at each layer\n",
    "    def linear_backward(self, dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "    \n",
    "        dW = (np.dot(dZ,A_prev.T))/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## backward propagation for activation step taken at each layer\n",
    "    def linear_activation_backward(self, dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "    \n",
    "        if activation == \"relu\":\n",
    "            dZ = self.__relu_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        elif activation == \"sigmoid\":\n",
    "            dZ = self.__sigmoid_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        elif activation == \"soft_max\":\n",
    "            dZ = self.__soft_max_backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    ## update parameters using gradient descent\n",
    "    def update_parameters(self, params, grads, learning_rate):\n",
    "        parameters = params.copy()\n",
    "        L = len(parameters) // 2 #num layers\n",
    "    \n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    ## runs the two layer NN \n",
    "    def two_layer_model(self, X, Y):\n",
    "        np.random.seed(1)\n",
    "        grads = {}\n",
    "        costs = []\n",
    "        m = X.shape[1]\n",
    "        (n_x, n_h, n_y) = self.dimensions\n",
    "        \n",
    "        parameters = self.initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        for i in range(0, self.num_iterations):\n",
    "            A1, cache1 = self.linear_activation_forward(X, W1, b1, self.hidden_function)\n",
    "            A2, cache2 = self.linear_activation_forward(A1, W2, b2, self.last_function)\n",
    "            cost = self.__compute_cost(A2, Y)\n",
    "            dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "            dA1, dW2, db2 = self.linear_activation_backward(dA2, cache2, self.last_function)\n",
    "            dA0, dW1, db1 = self.linear_activation_backward(dA1, cache1, self.hidden_function)\n",
    "            grads['dW1'] = dW1\n",
    "            grads['db1'] = db1\n",
    "            grads['dW2'] = dW2\n",
    "            grads['db2'] = db2\n",
    "            \n",
    "            parameters = self.update_parameters(parameters, grads, self.learning_rate)\n",
    "            \n",
    "            W1 = parameters[\"W1\"]\n",
    "            b1 = parameters[\"b1\"]\n",
    "            W2 = parameters[\"W2\"]\n",
    "            b2 = parameters[\"b2\"]\n",
    "        \n",
    "            if self.print_cost and (i % 100 == 0 or i == self.num_iterations - 1):\n",
    "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            if i % 100 == 0 or i == self.num_iterations:\n",
    "                costs.append(cost)\n",
    "            \n",
    "        return parameters, costs\n",
    "    \n",
    "    def plot_costs(self, costs):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "    def predict(self, parameters, X, Y, option=\"binary\"):\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        A1, cache1 = self.linear_activation_forward(X, W1, b1, self.hidden_function)\n",
    "        A2, cache2 = self.linear_activation_forward(A1, W2, b2, self.last_function)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if option == \"binary\":\n",
    "            A2 = np.round(A2)\n",
    "            num_rows, num_columns = A2.shape\n",
    "            for i in range(num_rows):\n",
    "                for j in range(num_columns):\n",
    "                    if A2[i][j] == Y[i][j]:\n",
    "                        correct += 1\n",
    "                    total +=1\n",
    "                \n",
    "        if option == \"multiclass\":\n",
    "            Y_arg = np.argmax(Y, axis = 0)\n",
    "            A2_arg = np.argmax(A2, axis = 0)\n",
    "            for i in range(len(Y_arg)):\n",
    "                if Y_arg[i] == A2_arg[i]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        print(\"total = \"+str(total)+\" correct = \"+str(correct))\n",
    "        print(\"Accuracy = \"+str(correct/total))\n",
    "        return correct/total\n",
    "    \n",
    "    def run_machine(self):\n",
    "        self.final_parameters, costs = self.two_layer_model(self.X, self.Y)\n",
    "        if self.plot_cost_bool:\n",
    "            self.plot_costs(costs)\n",
    "        \n",
    "    def machine_accuracy(self):\n",
    "        self.predict(self.final_parameters, self.X, self.Y, option=\"multiclass\")\n",
    "        self.predict(self.final_parameters, self.test_X, self.test_Y, option=\"multiclass\")\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a0079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
